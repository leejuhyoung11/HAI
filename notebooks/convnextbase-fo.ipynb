{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12030156,"sourceType":"datasetVersion","datasetId":7569184},{"sourceId":12083640,"sourceType":"datasetVersion","datasetId":7606792},{"sourceId":12159362,"sourceType":"datasetVersion","datasetId":7658010},{"sourceId":434630,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":354439,"modelId":375757},{"sourceId":434817,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":354600,"modelId":375926},{"sourceId":435378,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":355086,"modelId":376400},{"sourceId":435519,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":355203,"modelId":376516}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport os\nimport random\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport cv2\nfrom PIL import Image\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nfrom torch import nn, optim\nfrom torchvision.transforms import AutoAugment, AutoAugmentPolicy\nfrom torchvision.models import convnext_tiny, ConvNeXt_Tiny_Weights\n\nfrom torch.optim.lr_scheduler import OneCycleLR\n\n\nfrom sklearn.metrics import log_loss\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DEBUG = False\n\nif DEBUG: CFG = {\n    'IMG_SIZE': 232,\n    'BATCH_SIZE': 32,\n    'EPOCHS': 4,\n    'LEARNING_RATE': 1e-4,\n    'SEED' : 42\n}\nelse: CFG = {\n    'IMG_SIZE': 232,\n    'BATCH_SIZE': 32,\n    'EPOCHS': 50,\n    'LEARNING_RATE': 1e-4,\n    'SEED' : 42\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\nshutil.copytree('/kaggle/input/hai-competition/train', '/kaggle/working/hai-competition/train')\nshutil.copytree('/kaggle/input/train-cleaned/kaggle/working/hai-competition/train_cleaned', '/kaggle/working/hai-competition/train_cleaned')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_root1 = '/kaggle/working/hai-competition/train'\ntrain_root2 = '/kaggle/working/hai-competition/train_cleaned'\ntest_root = '/kaggle/input/hai-train/test'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"removing_data = [\n    '5ì‹œë¦¬ì¦ˆ_G60_2024_2025_0010.jpg', '6ì‹œë¦¬ì¦ˆ_GT_G32_2018_2020_0018.jpg', '7ì‹œë¦¬ì¦ˆ_G11_2016_2018_0040.jpg', '911_992_2020_2024_0030.jpg', 'E_í´ë˜ìŠ¤_W212_2010_2016_0022.jpg', 'K5_2ì„¸ëŒ€_2016_2018_0007.jpg', 'F150_2004_2021_0018.jpg', 'G_í´ë˜ìŠ¤_W463b_2019_2025_0030.jpg', 'GLE_í´ë˜ìŠ¤_W167_2019_2024_0068.jpg', 'Q5_FY_2021_2024_0032.jpg',\n    'Q30_2017_2019_0075.jpg', 'Q50_2014_2017_0031.jpg', 'SM7_ë‰´ì•„íŠ¸_2008_2011_0053.jpg', 'X3_G01_2022_2024_0029.jpg', 'XF_X260_2016_2020_0023.jpg', 'ë‰´_ES300h_2013_2015_0000.jpg', 'ë‰´_G80_2025_2026_0042.jpg', 'ë‰´_G80_2025_2026_0043.jpg', 'ë‰´_SM5_ì„í”„ë ˆì…˜_2008_2010_0033.jpg', 'ë”_ê¸°ì•„_ë ˆì´_EV_2024_2025_0078.jpg',\n    'ë”_ë‰´_K3_2ì„¸ëŒ€_2022_2024_0001.jpg', 'ë”_ë‰´_ê·¸ëœë“œ_ìŠ¤íƒ€ë ‰ìŠ¤_2018_2021_0078.jpg', 'ë”_ë‰´_ê·¸ëœë“œ_ìŠ¤íƒ€ë ‰ìŠ¤_2018_2021_0079.jpg', 'ë”_ë‰´_ê·¸ëœë“œ_ìŠ¤íƒ€ë ‰ìŠ¤_2018_2021_0080.jpg', 'ë”_ë‰´_ì•„ë°˜ë–¼_2014_2016_0031.jpg', 'ë”_ë‰´_íŒŒì‚¬íŠ¸_2012_2019_0067.jpg', 'ë ˆë‹ˆê²Œì´ë“œ_2019_2023_0041.jpg', 'ë°•ìŠ¤í„°_718_2017_2024_0011.jpg', 'ì‹¼íƒ€í˜_TM_2019_2020_0009.jpg', 'ì•„ë°˜ë–¼_MD_2011_2014_0081.jpg',\n    'ì•„ë°˜ë–¼_N_2022_2023_0064.jpg', 'ìµìŠ¤í”Œë¡œëŸ¬_2016_2017_0072.jpg', 'ì½°íŠ¸ë¡œí¬ë¥´í…Œ_2017_2022_0074.jpg', 'í”„ë¦¬ìš°ìŠ¤_4ì„¸ëŒ€_2019_2022_0052.jpg', 'ì•„ë°˜ë–¼_N_2022_2023_0035.jpg', 'E_í´ë˜ìŠ¤_W212_2010_2016_0069.jpg', 'ES300h_7ì„¸ëŒ€_2019_2026_0028.jpg', 'G_í´ë˜ìŠ¤_W463_2009_2017_0011.jpg', 'GLB_í´ë˜ìŠ¤_X247_2020_2023_0008.jpg', 'GLS_í´ë˜ìŠ¤_X167_2020_2024_0013.jpg',\n    'K3_2013_2015_0045.jpg', 'K5_3ì„¸ëŒ€_2020_2023_0081.jpg', 'Q7_4M_2020_2023_0011.jpg', 'RAV4_5ì„¸ëŒ€_2019_2024_0020.jpg', 'S_í´ë˜ìŠ¤_W223_2021_2025_0008.jpg', 'S_í´ë˜ìŠ¤_W223_2021_2025_0071.jpg', 'X4_F26_2015_2018_0068.jpg', 'ê·¸ëœë“œ_ì²´ë¡œí‚¤_WL_2021_2023_0018.jpg', 'ë ˆì´_2012_2017_0063.jpg', 'ë ˆì¸ì§€ë¡œë²„_5ì„¸ëŒ€_2023_2024_0030.jpg',\n    'ë ˆì¸ì§€ë¡œë²„_ìŠ¤í¬ì¸ _2ì„¸ëŒ€_2018_2022_0014.jpg', 'ë ˆì¸ì§€ë¡œë²„_ìŠ¤í¬ì¸ _2ì„¸ëŒ€_2018_2022_0017.jpg', 'ë§ˆì¹¸_2019_2021_0035.jpg', 'ë¨¸ìŠ¤íƒ±_2015_2023_0086.jpg', 'ì•„ë°˜ë–¼_MD_2011_2014_0009.jpg', 'ì•„ë°˜ë–¼_MD_2011_2014_0082.jpg', 'ì»¨í‹°ë„¨íƒˆ_GT_3ì„¸ëŒ€_2018_2023_0007.jpg', 'íƒ€ì´ì¹¸_2021_2025_0065.jpg', 'íŒŒë‚˜ë©”ë¼_2010_2016_0000.jpg', 'íŒŒë‚˜ë©”ë¼_2010_2016_0036.jpg',\n    '3ì‹œë¦¬ì¦ˆ_F30_2013_2018_0036.jpg', '4ì‹œë¦¬ì¦ˆ_F32_2014_2020_0027.jpg', '5ì‹œë¦¬ì¦ˆ_G60_2024_2025_0056.jpg', '7ì‹œë¦¬ì¦ˆ_F01_2009_2015_0029.jpg', '7ì‹œë¦¬ì¦ˆ_F01_2009_2015_0044.jpg', '911_992_2020_2024_0006.jpg', 'C_í´ë˜ìŠ¤_W204_2008_2015_0068.jpg', 'CLS_í´ë˜ìŠ¤_C257_2019_2023_0021.jpg',\n     '4ì‹œë¦¬ì¦ˆ_G22_2024_2025_0031.jpg', 'A_í´ë˜ìŠ¤_W177_2020_2025_0034.jpg', 'EQA_H243_2021_2024_0063.jpg', 'G_í´ë˜ìŠ¤_W463b_2019_2025_0049.jpg', 'SM7_ë‰´ì•„íŠ¸_2008_2011_0045.jpg',\n    'SM7_ë‰´ì•„íŠ¸_2008_2011_0067.jpg', 'SM7_ë‰´ì•„íŠ¸_2008_2011_0069.jpg', 'SM7_ë‰´ì•„íŠ¸_2008_2011_0083.jpg', 'SM7_ë‰´ì•„íŠ¸_2008_2011_0020.jpg', \n    'SM7_ë‰´ì•„íŠ¸_2008_2011_0001.jpg', 'X3_G01_2022_2024_0029.jpg', 'X7_G07_2019_2022_0052.jpg', 'XJ_8ì„¸ëŒ€_2010_2019_0064.jpg', 'YFì˜ë‚˜íƒ€_í•˜ì´ë¸Œë¦¬ë“œ_2011_2015_0003.jpg', \n    'YFì˜ë‚˜íƒ€_í•˜ì´ë¸Œë¦¬ë“œ_2011_2015_0072.jpg', 'YFì˜ë‚˜íƒ€_í•˜ì´ë¸Œë¦¬ë“œ_2011_2015_0013.jpg', 'YFì˜ë‚˜íƒ€_2009_2012_0026.jpg', 'YFì˜ë‚˜íƒ€_2009_2012_0068.jpg', \n    'YFì˜ë‚˜íƒ€_2009_2012_0045.jpg', 'ê·¸ëœì €TG_2007_2008_0022.jpg', 'ê·¸ëœì €TG_2007_2008_0023.jpg', 'ê·¸ëœì €TG_2007_2008_0075.jpg', 'ê·¸ëœì €TG_2007_2008_0008.jpg', 'ê·¸ëœì €TG_2007_2008_0009.jpg', \n    'ë‰´_A6_2012_2014_0046.jpg', 'ë‰´_GV80_2024_2025_0010.jpg', 'ë‰´_GV80_2024_2025_0021.jpg', 'ë‰´_GV80_2024_2025_0069.jpg', 'ë‰´_G80_2025_2026_0023.jpg', \n    'ë‰´_G80_2025_2026_0035.jpg', 'ë‰´_G80_2025_2026_0042.jpg', 'ë‰´_G80_2025_2026_0043.jpg', 'ë‰´_SM5_í”Œë˜í‹°ë„˜_2013_2014_0047.jpg', 'ë‰´_QM5_2012_2014_0001.jpg', 'ë‰´_QM5_2012_2014_0002.jpg', 'ë‰´ì˜ë Œí† _R_2013_2014_0009.jpg', \n    'ë”_ë‰´_QM6_2024_2025_0040.jpg', 'ë”_ë‰´_ìŠ¤íŒŒí¬_2019_2022_0040.jpg', 'ë”_ì˜¬ë‰´íˆ¬ì‹¼_í•˜ì´ë¸Œë¦¬ë“œ_2021_2023_0027.jpg', 'ë”_ì˜¬ë‰´íˆ¬ì‹¼_í•˜ì´ë¸Œë¦¬ë“œ_2021_2023_0038.jpg', \n    'ë”_ì˜¬ë‰´íˆ¬ì‹¼_í•˜ì´ë¸Œë¦¬ë“œ_2021_2023_0042.jpg', 'ë”_ì˜¬ë‰´G80_2021_2024_0001.jpg', 'ë”_ì˜¬ë‰´G80_2021_2024_0054.jpg', 'ë”_ì˜¬ë‰´G80_2021_2024_0070.jpg', \n    'ë”_ì˜¬ë‰´G80_2021_2024_0076.jpg', 'ë””_ì˜¬ë‰´ì½”ë‚˜_2023_2025_0058.jpg', 'ë ˆì¸ì§€ë¡œë²„_4ì„¸ëŒ€_2018_2022_0048.jpg', 'ë² ë‰´_2020_2024_0005.jpg', 'ë°•ìŠ¤í„°_718_2017_2024_0044.jpg', 'ë°•ìŠ¤í„°_718_2017_2024_0051.jpg', 'ë°•ìŠ¤í„°_718_2017_2024_0082.jpg', \n    'ì•„ë² ì˜¤_2012_2016_0018.jpg', 'ì•„ë² ì˜¤_2012_2016_0052.jpg', 'ì•„ë°˜ë–¼_N_2022_2023_0003.jpg', 'ì•„ë°˜ë–¼_N_2022_2023_0035.jpg', 'ì•„ë°˜ë–¼_N_2022_2023_0064.jpg', 'ì—ì¿ ìŠ¤_ì‹ í˜•_2010_2015_0044.jpg', 'ì¹´ì´ì—”_PO536_2019_2023_0035.jpg', \n    'í‹°ë³¼ë¦¬_ì—ì–´_2016_2019_0047.jpg', \n    \n]\n\nprint(len(removing_data))\n\ndeleted_count = 0\n\nfor p in removing_data:\n    path = os.path.join(train_root1, '_'.join(p.split('_')[:-1]), p)\n    if os.path.isfile(path):\n        try:\n            os.remove(path)\n            deleted_count += 1\n            # print(f\"Removed: {path}\")\n        except Exception as e:\n            print(f\"Error removing {path}: {e}\")\n    else:\n        print(f\"File not found: {path}\")\n\n    if train_root2:\n      path = os.path.join(train_root2, '_'.join(p.split('_')[:-1]), p)\n      if os.path.isfile(path):\n          try:\n              os.remove(path)\n              deleted_count += 1\n              # print(f\"Removed: {path}\")\n          except Exception as e:\n              print(f\"Error removing {path}: {e}\")\n      else:\n          print(f\"File not found: {path}\")\n\nprint(f\"\\nTotal deleted: {deleted_count} files.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomImageDataset(Dataset):\n    def __init__(self, root_dir1, root_dir2, transform=None, is_test=False):\n        self.root_dir1 = root_dir1\n        self.root_dir2 = root_dir2\n        self.transform = transform\n        self.is_test = is_test\n        self.samples = []\n        \n\n        if is_test:\n          \n            for fname in sorted(os.listdir(root_dir1)):\n                if fname.lower().endswith(('.jpg')):\n                    img_path = os.path.join(root_dir1, fname)\n                    self.samples.append((img_path,))\n        else:\n          \n            self.classes = sorted(os.listdir(root_dir1))\n            self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n            self.idx_to_class = {v: k for k, v in self.class_to_idx.items()}\n\n            # root 1            \n            for cls_name in self.classes:\n                cls_folder = os.path.join(root_dir1, cls_name)\n                model_label = self.class_to_idx[cls_name]\n               \n                for fname in os.listdir(cls_folder):\n                    if fname.lower().endswith(('.jpg')):\n                        img_path = os.path.join(cls_folder, fname)\n                        self.samples.append((img_path, model_label))\n\n            # root 2\n            if root_dir2:\n                for cls_name in self.classes:\n                    cls_folder = os.path.join(root_dir2, cls_name)\n                    model_label = self.class_to_idx[cls_name]\n                   \n                    for fname in os.listdir(cls_folder):\n                        if fname.lower().endswith(('.jpg')):\n                            img_path = os.path.join(cls_folder, fname)\n                            self.samples.append((img_path, model_label))\n\n    \n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        if self.is_test:\n            img_path = self.samples[idx][0]\n            image = Image.open(img_path).convert('RGB')\n            if self.transform:\n                image = self.transform(image)\n            return image\n        else:\n            img_path, model_label = self.samples[idx]\n            image = Image.open(img_path).convert('RGB')\n            if self.transform:\n                image = self.transform(image)\n            return image, model_label\n\n    def get_model_name(self, model_idx):\n      return self.idx_to_class[model_idx]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TestImageDataset(Dataset):\n    def __init__(self, root_dir, transform=None, is_test=False):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.is_test = is_test\n        self.samples = []\n        \n\n        if is_test:\n           \n            for fname in sorted(os.listdir(root_dir)):\n                if fname.lower().endswith(('.jpg')):\n                    img_path = os.path.join(root_dir, fname)\n                    self.samples.append((img_path,))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        if self.is_test:\n            img_path = self.samples[idx][0]\n            image = Image.open(img_path).convert('RGB')\n            if self.transform:\n                image = np.array(image)  # PIL â†’ np.ndarray\n                augmented = self.transform(image=image)\n                image = augmented['image']\n                \n            return image\n        else:\n            img_path, model_label = self.samples[idx]\n            image = Image.open(img_path).convert('RGB')\n            if self.transform:\n                image = np.array(image)  # PIL â†’ np.ndarray\n                augmented = self.transform(image=image)\n                image = augmented['image']\n                \n            return image, model_label\n\n    def get_model_name(self, model_idx):\n      return self.idx_to_class[model_idx]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def random_half_crop_horizontal(img,  **kwargs):\n    h, w, _ = img.shape\n    if np.random.rand() < 0.5:\n        return img[:, :w // 2, :]  \n    else:\n        return img[:, w // 2:, :] \n\ndef random_half_crop_vertical(img, **kwargs):\n    h, w, _ = img.shape\n    if np.random.rand() < 0.5:\n        return img[:h // 2, :, :] \n    else:\n        return img[h // 2:, :, :] ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\ntrain_transform = A.OneOf([\n   \n    A.Compose([\n        A.Resize(height=CFG['IMG_SIZE'], width=CFG['IMG_SIZE']),\n        A.Normalize(mean=(0.485, 0.456, 0.406),\n                    std=(0.229, 0.224, 0.225)),\n        ToTensorV2()\n    ], p=0.3),\n\n  \n    A.Compose([\n        A.OneOf([\n            A.Lambda(image=random_half_crop_horizontal),\n            A.Lambda(image=random_half_crop_vertical),\n        ], p=1.0),\n        A.SomeOf([ \n            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=1.0),\n            A.VerticalFlip(p=1.0),\n            A.HorizontalFlip(p=1.0),\n        ], n=2, replace=False),  # ë¬´ì‘ìœ„ë¡œ 2ê°œ ì„ íƒ\n\n        A.OneOf([\n            A.GaussianBlur(p=1.0),\n            A.CoarseDropout(p=1.0),\n            A.Sharpen(p=1.0),\n        ], p=0.5),\n\n        A.Resize(height=CFG['IMG_SIZE'], width=CFG['IMG_SIZE']),\n        A.Normalize(mean=(0.485, 0.456, 0.406),\n                    std=(0.229, 0.224, 0.225)),\n        ToTensorV2()\n    ], p=0.7)\n], p=1.0)\n\nval_transform = A.Compose([\n    A.Resize(height=CFG['IMG_SIZE'], width=CFG['IMG_SIZE']),\n    A.Normalize(mean=(0.485, 0.456, 0.406), \n                std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"full_dataset = CustomImageDataset(train_root1, train_root2, transform=None)\nprint(f\"Total num of images: {len(full_dataset)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"targets = [model for _, model in full_dataset.samples]\nclass_names = full_dataset.classes\n\n# Stratified Split\ntrain_idx, val_idx = train_test_split(\n    range(len(targets)), test_size=0.3, stratify=targets, random_state=42\n)\n\nclass TransformedSubset(Dataset):\n    def __init__(self, dataset, indices, transform):\n        self.dataset = dataset\n        self.indices = indices\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.indices)\n\n    def __getitem__(self, idx):\n        real_idx = self.indices[idx]\n        image, label = self.dataset[real_idx]\n        image = np.array(image)  # PIL â†’ np.ndarray\n        augmented = self.transform(image=image)\n        image = augmented['image']\n        return image, label\n\ntrain_dataset = TransformedSubset(full_dataset, train_idx, train_transform)\nval_dataset = TransformedSubset(full_dataset, val_idx, val_transform)\nprint(f'train ì´ë¯¸ì§€ ìˆ˜: {len(train_dataset)}, valid ì´ë¯¸ì§€ ìˆ˜: {len(val_dataset)}')\n\ntrain_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], num_workers=2, shuffle=True, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], num_workers=2, shuffle=False, pin_memory=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import timm\n\nnum_classes = len(full_dataset.class_to_idx)\n\nmodel = timm.create_model(\n    'convnextv2_base.fcmae_ft_in22k_in1k_384',\n    pretrained=True,\n    num_classes=num_classes\n)\n\nmodel = model.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_path = '/kaggle/working/'\n\nimport logging\n\nlog_path = os.path.join(save_path, 'train_log.txt')\nlogging.basicConfig(\n    level=logging.INFO,\n    format='[%(asctime)s] %(message)s',\n    handlers=[\n        logging.FileHandler(log_path),\n        logging.StreamHandler()\n    ]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport time\nimport torch\nimport torch.nn.functional as F\nimport logging\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.cuda.amp import GradScaler, autocast\nfrom sklearn.metrics import log_loss\nfrom tqdm import tqdm\n\ndef save_checkpoint(epoch, model, optimizer, scheduler, best_logloss, early_stop_counter, path):\n    checkpoint = {\n        'epoch': epoch,\n        'model_state': model.state_dict(),\n        'optimizer_state': optimizer.state_dict(),\n        'scheduler_state': scheduler.state_dict(),\n        'best_logloss': best_logloss,\n        'early_stop_counter': early_stop_counter\n    }\n    torch.save(checkpoint, path)\n\ndef load_checkpoint(model, optimizer, scheduler, path, device):\n    checkpoint = torch.load(path, map_location=device, weights_only=False)\n    model.load_state_dict(checkpoint['model_state'])\n    optimizer.load_state_dict(checkpoint['optimizer_state'])\n    scheduler.load_state_dict(checkpoint['scheduler_state'])\n    return checkpoint['epoch'] + 1, checkpoint['best_logloss'], checkpoint['early_stop_counter']\n\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=None, gamma=2, reduction='mean', label_smoothing=0.0):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n        self.label_smoothing = label_smoothing\n\n    def forward(self, logits, targets):\n        ce_loss = F.cross_entropy(logits, targets, reduction='none')\n        pt = torch.exp(-ce_loss)\n        at = self.alpha.gather(0, targets) if self.alpha is not None else 1.0\n        fl = at * (1 - pt) ** self.gamma * ce_loss\n        if self.reduction == 'mean':\n            return fl.mean()\n        elif self.reduction == 'sum':\n            return fl.sum()\n        else:\n            return fl\n\ntry:\n    samples = train_loader.dataset.dataset.samples\nexcept:\n    samples = train_loader.dataset.samples\nlabels = [label for _, label in samples]\ncls_counts = np.bincount(labels)\nalpha = torch.tensor(cls_counts.sum() / (len(cls_counts) * cls_counts), dtype=torch.float32).to(device)\n\n\ncriterion = FocalLoss(alpha=alpha, gamma=2.0, label_smoothing=0.1)\noptimizer = torch.optim.AdamW(model.parameters(), lr=CFG['LEARNING_RATE'])\nscheduler = OneCycleLR(\n    optimizer,\n    max_lr=CFG['LEARNING_RATE'],\n    steps_per_epoch=len(train_loader),\n    epochs=CFG['EPOCHS']\n)\nscaler = GradScaler()\n\nstart_epoch = 0\nbest_logloss = float('inf')\nearly_stop_counter = 0\npatience = 5\n\nresume_path = '/kaggle/input/epoch41/pytorch/default/1/checkpoint_epoch (41)_0.0094.pth'\nif os.path.exists(resume_path):\n    start_epoch, best_logloss, early_stop_counter = load_checkpoint(\n        model, optimizer, scheduler, resume_path, device\n    )\n    print(f\"Resumed from epoch {start_epoch}\")\n    \nfor param in model.parameters():\n            param.requires_grad = True\n\nfor epoch in range(start_epoch, CFG['EPOCHS']):\n\n    \n    if epoch == 0:\n        print(\"Freezing backbone\")\n        for name, param in model.named_parameters():\n            if 'head' not in name and 'classifier' not in name:\n                param.requires_grad = False\n    elif epoch == 10:\n        print(\"Unfreezing\")\n        for param in model.parameters():\n            param.requires_grad = True\n    \n    model.train()\n    train_loss = 0.0\n    correct_train = 0\n    total_train = 0\n    train_probs = []\n    train_true = []\n\n    for images, model_labels in tqdm(train_loader, desc=f\"[Epoch {epoch+1}/{CFG['EPOCHS']}] Training\", leave=True, dynamic_ncols=True):\n        images, model_labels = images.to(device), model_labels.to(device)\n\n        optimizer.zero_grad()\n        with autocast():\n            model_logits = model(images)\n            loss = criterion(model_logits, model_labels)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step()\n\n        train_loss += loss.item()\n        preds = model_logits.argmax(dim=1)\n        correct_train += (preds == model_labels).sum().item()\n        total_train += model_labels.size(0)\n        train_probs.extend(F.softmax(model_logits, dim=1).detach().cpu().numpy())\n        train_true.extend(model_labels.cpu().numpy())\n\n    avg_train_loss = train_loss / len(train_loader)\n    train_acc = 100 * correct_train / total_train\n    train_logloss = log_loss(train_true, train_probs, labels=list(range(len(full_dataset.class_to_idx))))\n\n    # ---------- VALID ----------\n    model.eval()\n    val_loss = 0.0\n    correct_val = 0\n    total_val = 0\n    val_probs = []\n    val_true = []\n\n    with torch.no_grad():\n        for images, model_labels in tqdm(val_loader, desc=f\"[Epoch {epoch+1}/{CFG['EPOCHS']}] Validation\"):\n            images, model_labels = images.to(device), model_labels.to(device)\n\n            model_logits = model(images)\n            loss = criterion(model_logits, model_labels)\n            val_loss += loss.item()\n\n            preds = model_logits.argmax(dim=1)\n            correct_val += (preds == model_labels).sum().item()\n            total_val += model_labels.size(0)\n            val_probs.extend(F.softmax(model_logits, dim=1).detach().cpu().numpy())\n            val_true.extend(model_labels.detach().cpu().numpy())\n\n    avg_val_loss = val_loss / len(val_loader)\n    val_acc = 100 * correct_val / total_val\n    val_logloss = log_loss(val_true, val_probs, labels=list(range(len(full_dataset.class_to_idx))))\n\n\n    print(f\"Train Loss : {avg_train_loss:.4f} || Valid Loss : {avg_val_loss:.4f}\")\n    print(f\"Train Acc  : {train_acc:.2f}% | Valid Acc  : {val_acc:.2f}%\")\n    print(f\"Train LogLoss: {train_logloss:.4f} | Valid LogLoss: {val_logloss:.4f}\")\n    \n    if val_logloss < best_logloss:\n        best_logloss = val_logloss\n        torch.save(model.state_dict(), os.path.join(save_path, 'best_model.pth'))\n        logging.info(f\"Best model saved at epoch {epoch+1} (Valid LogLoss: {val_logloss:.4f})\")\n        early_stop_counter = 0\n    else:\n        early_stop_counter += 1\n        logging.info(f\"Early stop counter: {early_stop_counter}/{patience}\")\n\n    save_checkpoint(\n        epoch, model, optimizer, scheduler,\n        best_logloss, early_stop_counter,\n        os.path.join(save_path, f'checkpoint_epoch.pth')\n    )\n\n    if early_stop_counter >= patience:\n        logging.info(f\"\\nğŸ›‘ Early stopping triggered at epoch {epoch+1}. Saving final model...\")\n        torch.save(model.state_dict(), os.path.join(save_path, 'final_model.pth'))\n        break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}