{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12030156,"sourceType":"datasetVersion","datasetId":7569184},{"sourceId":12083020,"sourceType":"datasetVersion","datasetId":7606438},{"sourceId":12083640,"sourceType":"datasetVersion","datasetId":7606792},{"sourceId":430029,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":350544,"modelId":371793}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport os\nimport random\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport cv2\nfrom PIL import Image\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nfrom torch import nn, optim\nfrom torchvision.transforms import AutoAugment, AutoAugmentPolicy\nfrom torchvision.models import convnext_tiny, ConvNeXt_Tiny_Weights\n\nfrom torch.optim.lr_scheduler import OneCycleLR\n\n\nfrom sklearn.metrics import log_loss\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DEBUG = False\n\nif DEBUG: CFG = {\n    'IMG_SIZE': 232,\n    'BATCH_SIZE': 32,\n    'EPOCHS': 4,\n    'LEARNING_RATE': 1e-4,\n    'SEED' : 42\n}\nelse: CFG = {\n    'IMG_SIZE': 232,\n    'BATCH_SIZE': 32,\n    'EPOCHS': 30,\n    'LEARNING_RATE': 1e-4,\n    'SEED' : 42\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\nshutil.copytree('/kaggle/input/hai-competition/train', '/kaggle/working/hai-competition/train')\nshutil.copytree('/kaggle/input/train-cleaned/kaggle/working/hai-competition/train_cleaned', '/kaggle/working/hai-competition/train_cleaned')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_root1 = '/kaggle/working/hai-competition/train'\ntrain_root2 = '/kaggle/working/hai-competition/train_cleaned'\ntest_root = '/kaggle/input/hai-competition/test'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"removing_data = [\n    '5시리즈_G60_2024_2025_0010.jpg', '6시리즈_GT_G32_2018_2020_0018.jpg', '7시리즈_G11_2016_2018_0040.jpg', '911_992_2020_2024_0030.jpg', 'E_클래스_W212_2010_2016_0022.jpg', 'K5_2세대_2016_2018_0007.jpg', 'F150_2004_2021_0018.jpg', 'G_클래스_W463b_2019_2025_0030.jpg', 'GLE_클래스_W167_2019_2024_0068.jpg', 'Q5_FY_2021_2024_0032.jpg',\n    'Q30_2017_2019_0075.jpg', 'Q50_2014_2017_0031.jpg', 'SM7_뉴아트_2008_2011_0053.jpg', 'X3_G01_2022_2024_0029.jpg', 'XF_X260_2016_2020_0023.jpg', '뉴_ES300h_2013_2015_0000.jpg', '뉴_G80_2025_2026_0042.jpg', '뉴_G80_2025_2026_0043.jpg', '뉴_SM5_임프레션_2008_2010_0033.jpg', '더_기아_레이_EV_2024_2025_0078.jpg',\n    '더_뉴_K3_2세대_2022_2024_0001.jpg', '더_뉴_그랜드_스타렉스_2018_2021_0078.jpg', '더_뉴_그랜드_스타렉스_2018_2021_0079.jpg', '더_뉴_그랜드_스타렉스_2018_2021_0080.jpg', '더_뉴_아반떼_2014_2016_0031.jpg', '더_뉴_파사트_2012_2019_0067.jpg', '레니게이드_2019_2023_0041.jpg', '박스터_718_2017_2024_0011.jpg', '싼타페_TM_2019_2020_0009.jpg', '아반떼_MD_2011_2014_0081.jpg',\n    '아반떼_N_2022_2023_0064.jpg', '익스플로러_2016_2017_0072.jpg', '콰트로포르테_2017_2022_0074.jpg', '프리우스_4세대_2019_2022_0052.jpg', '아반떼_N_2022_2023_0035.jpg', 'E_클래스_W212_2010_2016_0069.jpg', 'ES300h_7세대_2019_2026_0028.jpg', 'G_클래스_W463_2009_2017_0011.jpg', 'GLB_클래스_X247_2020_2023_0008.jpg', 'GLS_클래스_X167_2020_2024_0013.jpg',\n    'K3_2013_2015_0045.jpg', 'K5_3세대_2020_2023_0081.jpg', 'Q7_4M_2020_2023_0011.jpg', 'RAV4_5세대_2019_2024_0020.jpg', 'S_클래스_W223_2021_2025_0008.jpg', 'S_클래스_W223_2021_2025_0071.jpg', 'X4_F26_2015_2018_0068.jpg', '그랜드_체로키_WL_2021_2023_0018.jpg', '레이_2012_2017_0063.jpg', '레인지로버_5세대_2023_2024_0030.jpg',\n    '레인지로버_스포츠_2세대_2018_2022_0014.jpg', '레인지로버_스포츠_2세대_2018_2022_0017.jpg', '마칸_2019_2021_0035.jpg', '머스탱_2015_2023_0086.jpg', '아반떼_MD_2011_2014_0009.jpg', '아반떼_MD_2011_2014_0082.jpg', '컨티넨탈_GT_3세대_2018_2023_0007.jpg', '타이칸_2021_2025_0065.jpg', '파나메라_2010_2016_0000.jpg', '파나메라_2010_2016_0036.jpg',\n    '3시리즈_F30_2013_2018_0036.jpg', '4시리즈_F32_2014_2020_0027.jpg', '5시리즈_G60_2024_2025_0056.jpg', '7시리즈_F01_2009_2015_0029.jpg', '7시리즈_F01_2009_2015_0044.jpg', '911_992_2020_2024_0006.jpg', 'C_클래스_W204_2008_2015_0068.jpg', 'CLS_클래스_C257_2019_2023_0021.jpg',\n     '4시리즈_G22_2024_2025_0031.jpg', 'A_클래스_W177_2020_2025_0034.jpg', 'EQA_H243_2021_2024_0063.jpg', 'G_클래스_W463b_2019_2025_0049.jpg', 'SM7_뉴아트_2008_2011_0045.jpg',\n    'SM7_뉴아트_2008_2011_0067.jpg', 'SM7_뉴아트_2008_2011_0069.jpg', 'SM7_뉴아트_2008_2011_0083.jpg', 'SM7_뉴아트_2008_2011_0020.jpg', \n    'SM7_뉴아트_2008_2011_0001.jpg', 'X3_G01_2022_2024_0029.jpg', 'X7_G07_2019_2022_0052.jpg', 'XJ_8세대_2010_2019_0064.jpg', 'YF쏘나타_하이브리드_2011_2015_0003.jpg', \n    'YF쏘나타_하이브리드_2011_2015_0072.jpg', 'YF쏘나타_하이브리드_2011_2015_0013.jpg', 'YF쏘나타_2009_2012_0026.jpg', 'YF쏘나타_2009_2012_0068.jpg', \n    'YF쏘나타_2009_2012_0045.jpg', '그랜저TG_2007_2008_0022.jpg', '그랜저TG_2007_2008_0023.jpg', '그랜저TG_2007_2008_0075.jpg', '그랜저TG_2007_2008_0008.jpg', '그랜저TG_2007_2008_0009.jpg', \n    '뉴_A6_2012_2014_0046.jpg', '뉴_GV80_2024_2025_0010.jpg', '뉴_GV80_2024_2025_0021.jpg', '뉴_GV80_2024_2025_0069.jpg', '뉴_G80_2025_2026_0023.jpg', \n    '뉴_G80_2025_2026_0035.jpg', '뉴_G80_2025_2026_0042.jpg', '뉴_G80_2025_2026_0043.jpg', '뉴_SM5_플래티넘_2013_2014_0047.jpg', '뉴_QM5_2012_2014_0001.jpg', '뉴_QM5_2012_2014_0002.jpg', '뉴쏘렌토_R_2013_2014_0009.jpg', \n    '더_뉴_QM6_2024_2025_0040.jpg', '더_뉴_스파크_2019_2022_0040.jpg', '더_올뉴투싼_하이브리드_2021_2023_0027.jpg', '더_올뉴투싼_하이브리드_2021_2023_0038.jpg', \n    '더_올뉴투싼_하이브리드_2021_2023_0042.jpg', '더_올뉴G80_2021_2024_0001.jpg', '더_올뉴G80_2021_2024_0054.jpg', '더_올뉴G80_2021_2024_0070.jpg', \n    '더_올뉴G80_2021_2024_0076.jpg', '디_올뉴코나_2023_2025_0058.jpg', '레인지로버_4세대_2018_2022_0048.jpg', '베뉴_2020_2024_0005.jpg', '박스터_718_2017_2024_0044.jpg', '박스터_718_2017_2024_0051.jpg', '박스터_718_2017_2024_0082.jpg', \n    '아베오_2012_2016_0018.jpg', '아베오_2012_2016_0052.jpg', '아반떼_N_2022_2023_0003.jpg', '아반떼_N_2022_2023_0035.jpg', '아반떼_N_2022_2023_0064.jpg', '에쿠스_신형_2010_2015_0044.jpg', '카이엔_PO536_2019_2023_0035.jpg', \n    '티볼리_에어_2016_2019_0047.jpg', \n    \n]\n\nprint(len(removing_data))\n\ndeleted_count = 0\n\nfor p in removing_data:\n    path = os.path.join(train_root1, '_'.join(p.split('_')[:-1]), p)\n    if os.path.isfile(path):\n        try:\n            os.remove(path)\n            deleted_count += 1\n            \n        except Exception as e:\n            print(f\"Error removing {path}: {e}\")\n    else:\n        print(f\"File not found: {path}\")\n\n    if train_root2:\n      path = os.path.join(train_root2, '_'.join(p.split('_')[:-1]), p)\n      if os.path.isfile(path):\n          try:\n              os.remove(path)\n              deleted_count += 1\n              # print(f\"Removed: {path}\")\n          except Exception as e:\n              print(f\"Error removing {path}: {e}\")\n      else:\n          print(f\"File not found: {path}\")\n\nprint(f\"\\nTotal deleted: {deleted_count} files.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/hai-competition/test.csv')\nsubmission = pd.read_csv('/kaggle/input/hai-competition/sample_submission.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomImageDataset(Dataset):\n    def __init__(self, root_dir1, root_dir2, transform=None, is_test=False):\n        self.root_dir1 = root_dir1\n        self.root_dir2 = root_dir2\n        self.transform = transform\n        self.is_test = is_test\n        self.samples = []\n        \n\n        if is_test:\n            \n            for fname in sorted(os.listdir(root_dir1)):\n                if fname.lower().endswith(('.jpg')):\n                    img_path = os.path.join(root_dir1, fname)\n                    self.samples.append((img_path,))\n        else:\n            \n            self.classes = sorted(os.listdir(root_dir1))\n            self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n            self.idx_to_class = {v: k for k, v in self.class_to_idx.items()}\n\n            # root 1            \n            for cls_name in self.classes:\n                cls_folder = os.path.join(root_dir1, cls_name)\n                model_label = self.class_to_idx[cls_name]\n               \n                for fname in os.listdir(cls_folder):\n                    if fname.lower().endswith(('.jpg')):\n                        img_path = os.path.join(cls_folder, fname)\n                        self.samples.append((img_path, model_label))\n\n            # root 2\n            if root_dir2:\n                for cls_name in self.classes:\n                    cls_folder = os.path.join(root_dir2, cls_name)\n                    model_label = self.class_to_idx[cls_name]\n                   \n                    for fname in os.listdir(cls_folder):\n                        if fname.lower().endswith(('.jpg')):\n                            img_path = os.path.join(cls_folder, fname)\n                            self.samples.append((img_path, model_label))\n\n    \n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        if self.is_test:\n            img_path = self.samples[idx][0]\n            image = Image.open(img_path).convert('RGB')\n            if self.transform:\n                image = self.transform(image)\n            return image\n        else:\n            img_path, model_label = self.samples[idx]\n            image = Image.open(img_path).convert('RGB')\n            if self.transform:\n                image = self.transform(image)\n            return image, model_label\n\n    def get_model_name(self, model_idx):\n      return self.idx_to_class[model_idx]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TestImageDataset(Dataset):\n    def __init__(self, root_dir, transform=None, is_test=False):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.is_test = is_test\n        self.samples = []\n        \n\n        if is_test:\n        \n            for fname in sorted(os.listdir(root_dir)):\n                if fname.lower().endswith(('.jpg')):\n                    img_path = os.path.join(root_dir, fname)\n                    self.samples.append((img_path,))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        if self.is_test:\n            img_path = self.samples[idx][0]\n            image = Image.open(img_path).convert('RGB')\n            if self.transform:\n                image = np.array(image)  # PIL → np.ndarray\n                augmented = self.transform(image=image)\n                image = augmented['image']\n                \n            return image\n        else:\n            img_path, model_label = self.samples[idx]\n            image = Image.open(img_path).convert('RGB')\n            if self.transform:\n                image = np.array(image)  # PIL → np.ndarray\n                augmented = self.transform(image=image)\n                image = augmented['image']\n                \n            return image, model_label\n\n    def get_model_name(self, model_idx):\n      return self.idx_to_class[model_idx]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def random_half_crop_horizontal(img,  **kwargs):\n    h, w, _ = img.shape\n    if np.random.rand() < 0.5:\n        return img[:, :w // 2, :]  \n    else:\n        return img[:, w // 2:, :] \n\ndef random_half_crop_vertical(img, **kwargs):\n    h, w, _ = img.shape\n    if np.random.rand() < 0.5:\n        return img[:h // 2, :, :] \n    else:\n        return img[h // 2:, :, :] ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\ntrain_transform = A.OneOf([\n   \n    A.Compose([\n        A.Resize(height=CFG['IMG_SIZE'], width=CFG['IMG_SIZE']),\n        A.Normalize(mean=(0.485, 0.456, 0.406),\n                    std=(0.229, 0.224, 0.225)),\n        ToTensorV2()\n    ], p=1.0),\n\n   \n    A.Compose([\n        A.SomeOf([ \n            A.Lambda(image=random_half_crop_horizontal, p=1.0),\n            A.Lambda(image=random_half_crop_vertical, p=1.0),\n            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=1.0),\n            A.VerticalFlip(p=1.0),\n            A.HorizontalFlip(p=1.0),\n          \n        ], n=2, replace=False), \n\n        A.Resize(height=CFG['IMG_SIZE'], width=CFG['IMG_SIZE']),\n        A.Normalize(mean=(0.485, 0.456, 0.406),\n                    std=(0.229, 0.224, 0.225)),\n        ToTensorV2()\n    ], p=1.0)\n], p=1.0)\n\nval_transform = A.Compose([\n    A.Resize(height=CFG['IMG_SIZE'], width=CFG['IMG_SIZE']),\n    A.Normalize(mean=(0.485, 0.456, 0.406), \n                std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"full_dataset = CustomImageDataset(train_root1, train_root2, transform=None)\nprint(f\"Total num of images: {len(full_dataset)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"targets = [model for _, model in full_dataset.samples]\nclass_names = full_dataset.classes\n\n# Stratified Split\ntrain_idx, val_idx = train_test_split(\n    range(len(targets)), test_size=0.3, stratify=targets, random_state=42\n)\n\nclass TransformedSubset(Dataset):\n    def __init__(self, dataset, indices, transform):\n        self.dataset = dataset\n        self.indices = indices\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.indices)\n\n    def __getitem__(self, idx):\n        real_idx = self.indices[idx]\n        image, label = self.dataset[real_idx]\n        image = np.array(image)  # PIL → np.ndarray\n        augmented = self.transform(image=image)\n        image = augmented['image']\n        return image, label\n\ntrain_dataset = TransformedSubset(full_dataset, train_idx, train_transform)\nval_dataset = TransformedSubset(full_dataset, val_idx, val_transform)\nprint(f'train len: {len(train_dataset)}, valid len: {len(val_dataset)}')\n\n# DataLoader 정의\ntrain_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], num_workers=2, shuffle=True, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], num_workers=2, shuffle=False, pin_memory=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision.models import convnext_base, ConvNeXt_Base_Weights\n\nweights = ConvNeXt_Base_Weights.DEFAULT\nmodel = convnext_base(weights=weights)\n\n\nmodel.classifier[2] = nn.Linear(model.classifier[2].in_features, len(full_dataset.class_to_idx))\n\nmodel = model.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_path = '/kaggle/working/'\n\nimport logging\n\nlog_path = os.path.join(save_path, 'train_log.txt')\nlogging.basicConfig(\n    level=logging.INFO,\n    format='[%(asctime)s] %(message)s',\n    handlers=[\n        logging.FileHandler(log_path),\n        logging.StreamHandler()\n    ]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nimport os\n\nearly_stop_counter = 0\npatience = 3\nbest_logloss = float('inf')\n\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\noptimizer = torch.optim.AdamW(model.parameters(), lr=CFG['LEARNING_RATE'])\n\nscheduler = OneCycleLR(\n    optimizer,\n    max_lr=CFG['LEARNING_RATE'],\n    steps_per_epoch=len(train_loader),\n    epochs=CFG['EPOCHS']\n)\n\nfor epoch in range(CFG['EPOCHS']):\n    model.train()\n    train_loss = 0.0\n    correct_train = 0\n    total_train = 0\n    train_probs = []\n    train_true = []\n\n    for images, model_labels in tqdm(train_loader, desc=f\"[Epoch {epoch+1}/{CFG['EPOCHS']}] Training\", leave=True, dynamic_ncols=True):\n        images, model_labels = images.to(device), model_labels.to(device)\n\n        optimizer.zero_grad()\n        model_logits = model(images)\n        loss = criterion(model_logits, model_labels)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        train_loss += loss.item()\n        preds = model_logits.argmax(dim=1)\n        correct_train += (preds == model_labels).sum().item()\n        total_train += model_labels.size(0)\n        train_probs.extend(F.softmax(model_logits, dim=1).detach().cpu().numpy())\n        train_true.extend(model_labels.cpu().numpy())\n\n    avg_train_loss = train_loss / len(train_loader)\n    train_acc = 100 * correct_train / total_train\n    train_logloss = log_loss(train_true, train_probs, labels=list(range(len(full_dataset.class_to_idx))))\n\n    # ---------- VALID ----------\n    model.eval()\n    val_loss = 0.0\n    correct_val = 0\n    total_val = 0\n    val_probs = []\n    val_true = []\n\n    with torch.no_grad():\n        for images, model_labels in tqdm(val_loader, desc=f\"[Epoch {epoch+1}/{CFG['EPOCHS']}] Validation\"):\n            images, model_labels = images.to(device), model_labels.to(device)\n\n            model_logits = model(images)\n            loss = criterion(model_logits, model_labels)\n            val_loss += loss.item()\n\n            preds = model_logits.argmax(dim=1)\n            correct_val += (preds == model_labels).sum().item()\n            total_val += model_labels.size(0)\n            val_probs.extend(F.softmax(model_logits, dim=1).detach().cpu().numpy())\n            val_true.extend(model_labels.detach().cpu().numpy())\n\n    avg_val_loss = val_loss / len(val_loader)\n    val_acc = 100 * correct_val / total_val\n    val_logloss = log_loss(val_true, val_probs, labels=list(range(len(full_dataset.class_to_idx))))\n\n    print(f\"\\n Epoch {epoch+1} Summary\")\n    print(f\"Train Loss : {avg_train_loss:.4f} || Valid Loss : {avg_val_loss:.4f}\")\n    print(f\"Train Acc  : {train_acc:.2f}% | Valid Acc  : {val_acc:.2f}%\")\n    print(f\"Train LogLoss: {train_logloss:.4f} | Valid LogLoss: {val_logloss:.4f}\")\n\n   \n    if val_logloss < best_logloss:\n        best_logloss = val_logloss\n        torch.save(model.state_dict(), os.path.join(save_path, 'best_model.pth'))\n        logging.info(f\"Best model saved at epoch {epoch+1} (Valid LogLoss: {val_logloss:.4f})\")\n        early_stop_counter = 0\n    else:\n        early_stop_counter += 1\n        logging.info(f\"⏸ No improvement. Early stop counter: {early_stop_counter}/{patience}\")\n\n\n    if early_stop_counter >= patience:\n        logging.info(f\"\\nEarly stopping at epoch {epoch+1}. Saving final model...\")\n        torch.save(model.state_dict(), os.path.join(save_path, 'final_model.pth'))\n        break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tta_transforms = [\n    A.Compose([\n        A.HorizontalFlip(p=1.0),\n        A.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE']),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2()\n    ]),\n    A.Compose([\n        A.VerticalFlip(p=1.0),\n        A.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE']),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2()\n    ]),\n    A.Compose([\n        A.RandomBrightnessContrast(p=1.0),\n        A.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE']),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2()\n    ]),\n    A.Compose([  # identity transform\n        A.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE']),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2()\n    ]),\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = TestImageDataset(test_root, transform=val_transform, is_test=True)\ntest_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_path = '/kaggle/input/convnextbase_ob_best/pytorch/default/1/convnext_OB_best_model.pth'\n\nmodel.load_state_dict(torch.load(model_path, map_location=device))\nmodel.to(device);","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\nmodel_results = np.zeros((len(test_dataset), len(submission.columns[1:])))\n\nwith torch.no_grad():\n    for tta in tta_transforms:\n        \n        tta_dataset = TestImageDataset(test_root, transform=tta, is_test=True)\n        tta_loader = DataLoader(tta_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)\n\n        start_idx = 0\n        for images in tqdm(tta_loader, desc=\"Predicting with TTA\"):\n            images = images.to(device)\n            logits = model(images)\n            probs = F.softmax(logits, dim=1).cpu().numpy()\n\n            if start_idx == 0: print(probs)\n\n            batch_size = images.size(0)\n            model_results[start_idx:start_idx + batch_size] += probs\n            start_idx += batch_size\n\nmodel_results /= len(tta_transforms)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/hai-competition/test.csv')\nsubmission = pd.read_csv('/kaggle/input/hai-competition/sample_submission.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_columns = submission.columns[1:]\nsubmission[class_columns] = model_results\n\nsubmission.to_csv(save_path + 'final_submission.csv', index=False, encoding='utf-8-sig')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}